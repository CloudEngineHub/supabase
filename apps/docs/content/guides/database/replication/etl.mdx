---
title: ETL
description: Replicate your Supabase database to external destinations
---

ETL simplifies the process of syncing your data to an external destination. This could be another database, a data warehouse, or a specialized service such as customer management or business analytics software.

<Admonition type="caution">

`ETL` is currently in alpha.

</Admonition>

Syncing requires a source and a destination. The source is your Supabase database, defined in the [**Publications**](/dashboard/project/_/database/publications) section of the dashboard.

The destination is a third party location, defined in the [**Replication**](/dashboard/project/_/database/replication) section of the dashboard. See the [supported destinations](#destinations).

## Sources

Choose what data to sync by creating a publication in your database. A publication is a set of changes, filtered by table and change type.

### Managing publications

1. Go to the [Database](/dashboard/project/_/database/tables) page in the Dashboard.
2. Click on **Publications** in the sidebar.
3. To work with ETL, toggle **all** database events.
4. Control which tables broadcast changes by selecting **Source** and toggling each table.

<video width="99%" muted playsInline controls={true}>
  <source
    src="https://xguihxuzqibwxjnimxev.supabase.co/storage/v1/object/public/videos/docs/api/api-realtime.mp4"
    type="video/mp4"
  />
</video>

## Destinations

Currently available destinations:

- [BigQuery](#bigquery)

### BigQuery

#### Prerequisites

Before setting up replication to BigQuery, you need:

1. A [Google Cloud Platform (GCP)](https://cloud.google.com/gcp) account
2. A [BigQuery dataset](https://cloud.google.com/bigquery/docs/datasets-intro) in your GCP project
3. A [GCP service account key](https://cloud.google.com/iam/docs/keys-create-delete) in JSON format

#### Configuring BigQuery replication

1. Navigate to the [Database](/dashboard/project/_/database/tables) section in your Supabase Dashboard and select the **Replication** tab.
2. Click **Add destination** to open the New Destination panel.
3. Configure the replication settings:
   - **Name**: Enter a descriptive name for your replication destination
   - **Publication**: Select an existing publication or create a new one
   - **Type**: Select "BigQuery" from the destination types
   - **Project Id**: Enter the GCP project ID
   - **Project's Dataset Id**: Enter the ID of the BigQuery Data set

    <Admonition type="note">

      The BigQuery Data set ID shows the project ID and Data set ID as one string, separated by a ".". The Data set ID is the part after the ".".
    
    </Admonition>

   - **Service Account Key**: Enter your GCP service account key
4. Review the [Advanced Settings](#advanced-settings) to change the limits for the replication.

   - **Max size**: Changes are sent in batches. Max size limits the size of an individual batch.
   - **Max fill seconds**: Number of seconds after which a batch should be sent, even if it hasn't reached the max size.
   - **Max staleness**: See BigQuery's [`max_staleness`](https://cloud.google.com/bigquery/docs/change-data-capture#manage_table_staleness) option

5. Click **Create** to start the replication process.

<Admonition type="note">

Due to ingestion latency in BigQuery, there may be a delay in the data flow.

</Admonition>

## Monitoring replication

After setting up replication, data begins flowing within a few minutes. You can monitor the replication status and review any errors directly from the [Supabase Dashboard](https://supabase.com/dashboard/project/_/database/replication) and clicking the **View status** button for a replication. In status view for the replication pipeline, you can also view logs by clicking the **View logs** button.

## Limitations

- Large values stored as [TOASTed values](https://www.postgresql.org/docs/current/storage-toast.html) are replicated, but limited to 10MB.
- Custom data types are not supported.
- No support for picking event types to replicate. ETL needs all insert, update and delete events.
- No column lists and row filter conditions support.
- No partitioned tables support.
- No generated columns support.
- No support for changing schema in the source.
- Tables should have a primary key set.
